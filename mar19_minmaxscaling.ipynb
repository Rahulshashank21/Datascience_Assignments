{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308c0e22-2732-414f-a491-31d2f0bf120c",
   "metadata": {},
   "source": [
    "# Q1. **What is Min-Max Scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "**Min-Max Scaling** is a data preprocessing technique used to scale numeric features to a specific range, typically between 0 and 1. It works by linearly transforming the original data into a new range based on the minimum and maximum values of the feature.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\\[ \\text{Scaled Value} = \\frac{\\text{Original Value} - \\text{Minimum Value}}{\\text{Maximum Value} - \\text{Minimum Value}} \\]\n",
    "\n",
    "Example:\n",
    "Let's say we have a dataset of exam scores ranging from 60 to 100, and we want to scale these values to a range of 0 to 1 using Min-Max scaling.\n",
    "\n",
    "Original exam scores: \\[70, 80, 90, 60, 100\\]\n",
    "\n",
    "Min-Max Scaling:\n",
    "\\[ \\text{Scaled Value} = \\frac{\\text{Original Value} - 60}{100 - 60} \\]\n",
    "\n",
    "After scaling, the values will be:\n",
    "\\[ \\frac{70 - 60}{100 - 60} = 0.2 \\]\n",
    "\\[ \\frac{80 - 60}{100 - 60} = 0.4 \\]\n",
    "\\[ \\frac{90 - 60}{100 - 60} = 0.6 \\]\n",
    "\\[ \\frac{60 - 60}{100 - 60} = 0 \\]\n",
    "\\[ \\frac{100 - 60}{100 - 60} = 1 \\]\n",
    "\n",
    "So, the scaled exam scores will be: \\[0.2, 0.4, 0.6, 0, 1\\]\n",
    "\n",
    "# Q2. **What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max Scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "**Unit Vector Scaling**, also known as **Normalization**, is a data preprocessing technique used to scale features to have unit norm. In this method, each data point is scaled independently to have a magnitude of 1.\n",
    "\n",
    "The formula for Unit Vector Scaling is as follows:\n",
    "\\[ \\text{Scaled Value} = \\frac{\\text{Original Value}}{\\|\\text{Original Value}\\|} \\]\n",
    "\n",
    "Where \\(\\|\\text{Original Value}\\|\\) represents the Euclidean norm (magnitude) of the original value.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "The primary difference is that Min-Max Scaling scales data to a specific range (e.g., 0 to 1), while Unit Vector Scaling scales data to have unit norm, meaning they will fall on the unit circle in multi-dimensional space.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset with two features, \\(\\text{Feature A}\\) and \\(\\text{Feature B}\\), and we want to apply Unit Vector Scaling to this dataset.\n",
    "\n",
    "Original data:\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Feature A} & : [3, 4] \\\\\n",
    "\\text{Feature B} & : [1, 2]\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Unit Vector Scaling:\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Scaled Feature A} & : \\left[ \\frac{3}{\\sqrt{3^2 + 4^2}}, \\frac{4}{\\sqrt{3^2 + 4^2}} \\right] \\\\\n",
    "\\text{Scaled Feature B} & : \\left[ \\frac{1}{\\sqrt{1^2 + 2^2}}, \\frac{2}{\\sqrt{1^2 + 2^2}} \\right]\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "After normalization, the scaled data will be:\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Scaled Feature A} & : \\left[ \\frac{3}{5}, \\frac{4}{5} \\right] \\\\\n",
    "\\text{Scaled Feature B} & : \\left[ \\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}} \\right]\n",
    "\\end{align*}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecb2b2-30d4-4d02-bb15-386f6c3b8923",
   "metadata": {},
   "source": [
    "# Q3. **What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining most of the important information.\n",
    "\n",
    "The main steps in PCA are as follows:\n",
    "1. **Standardize the data**: Scale the features to have zero mean and unit variance.\n",
    "2. **Compute the covariance matrix**: Calculate the covariance matrix of the standardized data.\n",
    "3. **Perform eigendecomposition**: Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Select principal components**: Sort the eigenvalues in descending order and choose the top k eigenvectors as principal components, where k is the desired lower dimensionality.\n",
    "5. **Project the data**: Multiply the original data by the selected eigenvectors to obtain the lower-dimensional representation.\n",
    "\n",
    "**Example:**\n",
    "Let's consider a dataset with two features, Feature A and Feature B, and we want to apply PCA to reduce the dimensionality to one dimension.\n",
    "\n",
    "Original data:\n",
    "Feature A : [2, 4, 5, 7, 9]\n",
    "Feature B : [3, 6, 2, 5, 8]\n",
    "\n",
    "Step 1: Standardize the data\n",
    "Feature A (standardized) : [-1.663, -0.554, -0.184, 0.735, 1.666]\n",
    "Feature B (standardized) : [-1.473, -0.295, -1.663, -0.926, 0.856]\n",
    "\n",
    "Step 2: Compute the covariance matrix\n",
    "Covariance Matrix = [[1.000, 0.934], [0.934, 1.000]]\n",
    "\n",
    "Step 3: Perform eigendecomposition\n",
    "Eigenvalues : [1.934, 0.066]\n",
    "Eigenvectors : [[0.707, -0.707], [0.707, 0.707]]\n",
    "\n",
    "Step 4: Select principal components\n",
    "Since we want to reduce to one dimension, we choose the eigenvector corresponding to the highest eigenvalue:\n",
    "Selected Eigenvector : [0.707, 0.707]\n",
    "\n",
    "Step 5: Project the data\n",
    "The lower-dimensional representation can be obtained by multiplying the original data by the selected eigenvector:\n",
    "Projected Data = [2*0.707, 4*0.707, 5*0.707, 7*0.707, 9*0.707]\n",
    "Projected Data = [1.414, 2.828, 3.536, 4.949, 6.364]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28c30a-924a-4d3f-893c-ee8aff6f428c",
   "metadata": {},
   "source": [
    "# Q4. **What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "**PCA** can be used for **Feature Extraction** to reduce the dimensionality of the data while retaining most of the information. In PCA, the original features are transformed into a new set of features called **Principal Components**. These principal components are orthogonal to each other and are ranked in order of importance, with the first component capturing the most variance in the data.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with three features, Feature A, Feature B, and Feature C. We want to perform feature extraction using PCA to reduce the dimensionality to two dimensions.\n",
    "\n",
    "Original data:\n",
    "Feature A : [3, 5, 2, 8, 6]\n",
    "Feature B : [4, 2, 9, 5, 7]\n",
    "Feature C : [1, 6, 3, 2, 5]\n",
    "\n",
    "Step 1: Standardize the data (mean = 4.8, std dev = 2.315)\n",
    "Feature A (standardized) : [-1.075, 0.323, -1.410, 2.580, 0.581]\n",
    "Feature B (standardized) : [-0.861, -1.609, 1.722, -0.538, 0.286]\n",
    "Feature C (standardized) : [-1.505, 0.968, -0.966, -1.410, -0.087]\n",
    "\n",
    "Step 2: Compute the covariance matrix\n",
    "Covariance Matrix = [[3.000, -0.667, 0.200], [-0.667, 8.000, 0.667], [0.200, 0.667, 7.000]]\n",
    "\n",
    "Step 3: Perform eigendecomposition\n",
    "Eigenvalues : [8.927, 1.492, 7.581]\n",
    "Eigenvectors : [[-0.243, -0.923, -0.299], [-0.781, 0.374, -0.500], [-0.575, 0.083, 0.814]]\n",
    "\n",
    "Step 4: Select principal components\n",
    "Since we want to reduce to two dimensions, we choose the two eigenvectors corresponding to the highest eigenvalues:\n",
    "Selected Eigenvectors : [[-0.243, -0.923], [-0.781, 0.374]]\n",
    "\n",
    "Step 5: Project the data\n",
    "The lower-dimensional representation can be obtained by multiplying the original data by the selected eigenvectors:\n",
    "Projected Data = [[-1.833, -0.246], [-0.676, -0.502], [-1.233, 0.572], [2.259, -1.039], [1.542, 1.214]]\n",
    "\n",
    "# Q5. **You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "In the context of a recommendation system, preprocessing the data using Min-Max scaling is essential to ensure that all the features are in the same range. This is important because some machine learning algorithms can be sensitive to the scale of features and may give more weight to features with larger values.\n",
    "\n",
    "To use Min-Max scaling for the food delivery dataset, we would follow these steps:\n",
    "1. Identify the features to be scaled, such as price, rating, and delivery time.\n",
    "2. Calculate the minimum and maximum values for each feature in the dataset.\n",
    "3. Apply the Min-Max scaling formula to scale each feature to a range between 0 and 1.\n",
    "\n",
    "For example, let's say the original values for the features are as follows:\n",
    "Price : [10, 20, 15, 25, 30]\n",
    "Rating : [3.5, 4.2, 3.9, 4.8, 4.0]\n",
    "Delivery Time : [30, 20, 25, 15, 35]\n",
    "\n",
    "After applying Min-Max scaling, the scaled values will be:\n",
    "Scaled Price : [0.125, 0.375, 0.250, 0.625, 0.875]\n",
    "Scaled Rating : [0.375, 0.700, 0.550, 1.000, 0.625]\n",
    "Scaled Delivery Time : [0.375, 0.125, 0.250, 0.000, 0.500]\n",
    "\n",
    "# Q6. **You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "When dealing with a large number of features in a stock price prediction dataset, PCA can be used to reduce the dimensionality and extract the most important information. This can help in avoiding the curse of dimensionality, improving model training time, and reducing the risk of overfitting.\n",
    "\n",
    "The steps to use PCA for dimensionality reduction in the stock price dataset are as follows:\n",
    "1. Standardize the data to have zero mean and unit variance. This step is crucial as PCA is sensitive to the scale of features.\n",
    "2. Compute the covariance matrix of the standardized data.\n",
    "3. Perform eigendecomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "4. Sort the eigenvalues in descending order to rank the principal components by importance.\n",
    "5. Select the top k eigenvectors corresponding to the k largest eigenvalues, where k is the desired lower dimensionality.\n",
    "6. Project the original data onto the selected k eigenvectors to obtain the lower-dimensional representation.\n",
    "\n",
    "By reducing the dimensionality with PCA, we obtain a new set of features (principal components) that capture the most significant information from the original dataset.\n",
    "\n",
    "# Q7. **For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**\n",
    "\n",
    "To perform Min-Max scaling to a range of -1 to 1, we need to follow these steps:\n",
    "1. Calculate the minimum and maximum values in the dataset.\n",
    "2. Apply the Min-Max scaling formula to scale each value to the desired range.\n",
    "\n",
    "For the given dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "Step 1: Calculate minimum and maximum\n",
    "Minimum value = 1\n",
    "Maximum value = 20\n",
    "\n",
    "Step 2: Apply Min-Max scaling formula\n",
    "Scaled Value = (Original Value - Minimum Value) / (Maximum Value - Minimum Value)\n",
    "\n",
    "Scaled dataset:\n",
    "\\[ -1.000, -0.500, 0.000, 0.500, 1.000 \\]\n",
    "\n",
    "# Q8. **For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "The number of principal components to retain in PCA depends on the desired trade-off between dimensionality reduction and information preservation. Typically, we aim to retain enough principal components to explain a significant portion of the variance in the data.\n",
    "\n",
    "The steps to perform Feature Extraction using PCA are as follows:\n",
    "1. Standardize the data to have zero mean and unit variance.\n",
    "2. Compute the covariance matrix of the standardized data.\n",
    "3. Perform eigendecomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "4. Sort the eigenvalues in descending order to rank the principal components by importance.\n",
    "5. Select the top k eigenvectors corresponding to the k largest eigenvalues, where k is the desired lower dimensionality.\n",
    "\n",
    "To determine the number of principal components to retain, we can analyze the explained variance ratio. This represents the proportion of the total variance explained by each principal component. We typically aim to retain enough principal components to explain, for example, 95% or 99% of the total variance.\n",
    "\n",
    "For instance, we may find that the first three principal components explain 90% of the variance, while the first five explain 95%. In such a scenario, we would choose to retain five principal components to preserve a significant amount of information while reducing the dimensionality of the dataset.\n",
    "\n",
    "Note: The exact number of principal components to retain may vary based on the specific dataset and the requirements of the modeling task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89a25c-a384-40c9-9c5f-6e8a4a03e298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
