{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in predicting the quality of wine.\n\nThe wine quality dataset consists of two datasets: one for red wine and one for white wine. The key features include fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol. These features can be used to predict the quality of wine as they represent the chemical properties of the wine. For example, acidity affects the flavor and balance of wine, while sugar content affects the sweetness. Sulphates can impact the wine's stability and aging potential, while alcohol affects the body and warmth of the wine.\n\nQ2. How did you handle missing data in the wine quality data set during the feature engineering process? Discuss the advantages and disadvantages of different imputation techniques.\n\nThere are several techniques to handle missing data in the wine quality dataset, such as mean imputation, median imputation, and interpolation. In my analysis, I used mean imputation as it is a simple method and can work well when the missing data is randomly distributed. However, mean imputation can lead to biased estimates if the data is not missing at random. Median imputation can be a better alternative in this case as it is robust to outliers. Interpolation can also be useful when dealing with time series data where the missing values are related to neighboring values. However, it can introduce errors if the pattern of the missing values is not clear.\n\nQ3. What are the key factors that affect students' performance in exams? How would you go about analyzing these factors using statistical techniques?\n\nThe key factors that affect students' performance in exams include student demographics, prior academic achievement, study habits, and classroom environment. To analyze these factors, we can use statistical techniques such as regression analysis to identify the relationship between the dependent variable (exam performance) and independent variables (student demographics, prior academic achievement, study habits, and classroom environment). We can also use exploratory data analysis techniques to identify patterns and trends in the data, such as box plots and scatterplots. Additionally, we can use machine learning techniques such as decision trees and random forests to identify the most important predictors of exam performance.\n\nQ4. Describe the process of feature engineering in the context of the student performance data set. How did you select and transform the variables for your model?\n\nIn the context of the student performance dataset, feature engineering involves selecting and transforming the variables to create new features that may be more informative for predicting exam performance. For example, we can transform the student's age into a categorical variable representing different age groups. We can also create a new variable that represents the student's average grade across all their courses. Additionally, we can create interaction terms between variables, such as the product of a student's study hours and their prior academic achievement. To select the most relevant features, we can use statistical techniques such as correlation analysis and feature importance analysis from machine learning models. Finally, we can use dimensionality reduction techniques such as principal component analysis to reduce the number of features while retaining the most important information.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\nof each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\nthese features to improve normality?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Wine Quality dataset\nwine_data = pd.read_csv('winequality.csv')\n\n# Plot histogram of each feature\nfig, axes = plt.subplots(nrows=2, ncols=6, figsize=(20, 6))\nfor i, ax in enumerate(axes.flatten()):\n    if i < len(wine_data.columns):\n        sns.histplot(wine_data.iloc[:, i], ax=ax)\n        ax.set_title(wine_data.columns[i])\nplt.tight_layout()\nplt.show()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\nfeatures. What is the minimum number of principal components required to explain 90% of the variance in\nthe data?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\n# Standardize the data\nscaler = StandardScaler()\nwine_data_scaled = scaler.fit_transform(wine_data)\n\n\nfrom sklearn.decomposition import PCA\n\n# Apply PCA\npca = PCA()\npca.fit(wine_data_scaled)\n\n# Calculate explained variance ratio\nexplained_var = pca.explained_variance_ratio_\n\n\n# Calculate cumulative explained variance ratio\ncumulative_var = np.cumsum(explained_var)\n\n# Find the minimum number of principal components required to explain 90% of the variance\nmin_components = np.argmax(cumulative_var >= 0.9) + 1\n\n\n# Plot explained variance ratio and cumulative explained variance ratio\nfig, ax = plt.subplots(figsize=(8, 4))\n\nsns.barplot(x=np.arange(1, len(explained_var)+1), y=explained_var, color='b', ax=ax)\nsns.lineplot(x=np.arange(1, len(explained_var)+1), y=cumulative_var, color='r', ax=ax)\n\nax.set_title(\"Explained Variance Ratio and Cumulative Explained Variance Ratio\")\nax.set_xlabel(\"Principal Component\")\nax.set_ylabel(\"Explained Variance Ratio\")\n\nplt.show()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}